# ðŸ”§ Guia de Troubleshooting

> DiagnÃ³stico e resoluÃ§Ã£o de problemas no Jimi IoT Gateway

## ðŸ“‹ Ãndice

- [CenÃ¡rio: Dados Atrasando](#-cenÃ¡rio-dados-da-jimi-cloud-comeÃ§am-a-atrasar)
- [Checklist de DiagnÃ³stico](#-checklist-de-diagnÃ³stico)
- [MÃ©tricas-Chave](#-mÃ©tricas-chave)
- [EstratÃ©gias de ResoluÃ§Ã£o](#-estratÃ©gias-de-resoluÃ§Ã£o)
- [Comandos de Troubleshooting](#-comandos-de-troubleshooting)
- [Runbooks](#-runbooks)

---

## ðŸš¨ CenÃ¡rio: Dados da Jimi Cloud ComeÃ§am a Atrasar

### Sintomas

- Dispositivos IoT nÃ£o atualizam posiÃ§Ã£o em tempo real
- Alertas chegam com atraso
- Heartbeats nÃ£o sÃ£o processados dentro do SLA

### DiagnÃ³stico SistemÃ¡tico

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FLUXO DE DIAGNÃ“STICO                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  1. VERIFICAR SAÃšDE DOS CONTAINERS                                   â”‚
â”‚     â””â”€â–º docker compose ps                                            â”‚
â”‚         â””â”€â–º Algum container estÃ¡ "unhealthy" ou "restarting"?       â”‚
â”‚                                                                      â”‚
â”‚  2. VERIFICAR RECURSOS (CPU/RAM)                                     â”‚
â”‚     â””â”€â–º docker stats                                                 â”‚
â”‚         â””â”€â–º CPU > 80%? MemÃ³ria > 90%?                               â”‚
â”‚                                                                      â”‚
â”‚  3. ANALISAR TAXA DE ERRO HTTP                                       â”‚
â”‚     â””â”€â–º Prometheus: rate(jimi_http_errors_total[5m])                â”‚
â”‚         â””â”€â–º Taxa > 5%? Investigar logs                              â”‚
â”‚                                                                      â”‚
â”‚  4. VERIFICAR LATÃŠNCIA                                               â”‚
â”‚     â””â”€â–º Prometheus: histogram_quantile(0.95, ...)                   â”‚
â”‚         â””â”€â–º P95 > 500ms? Gargalo identificado                       â”‚
â”‚                                                                      â”‚
â”‚  5. ANALISAR LOGS                                                    â”‚
â”‚     â””â”€â–º Grafana/Loki: {job="backend"} |= "error"                    â”‚
â”‚         â””â”€â–º Identificar padrÃ£o de erros                             â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ… Checklist de DiagnÃ³stico

### 1. Verificar SaÃºde dos Containers

```bash
# Status de todos os containers
docker compose ps

# SaÃ­da esperada (todos "healthy"):
# NAME              STATUS                   PORTS
# jimi-nginx        Up (healthy)             0.0.0.0:80->80/tcp, 0.0.0.0:443->443/tcp
# jimi-backend      Up (healthy)             8000/tcp
# jimi-prometheus   Up (healthy)             0.0.0.0:9090->9090/tcp
# jimi-grafana      Up (healthy)             0.0.0.0:3000->3000/tcp
# jimi-loki         Up (healthy)             0.0.0.0:3100->3100/tcp
```

### 2. Monitorar CPU e MemÃ³ria

```bash
# EstatÃ­sticas em tempo real
docker stats --no-stream

# MÃ©tricas detalhadas
docker stats --format "table {{.Name}}\t{{.CPUPerc}}\t{{.MemUsage}}\t{{.MemPerc}}"
```

**Thresholds de Alerta:**
| MÃ©trica | Warning | Critical |
|---------|---------|----------|
| CPU | > 70% | > 90% |
| MemÃ³ria | > 80% | > 95% |

### 3. Analisar Taxa de Erro HTTP

**Nota:** Prometheus e Loki nÃ£o expÃµem portas no host. Use o backend como ponto de acesso interno com `docker compose exec backend`.

```bash
# Via Prometheus API
docker compose exec backend curl -s "http://prometheus:9090/api/v1/query" \
  --data-urlencode 'query=sum(rate(jimi_http_errors_total[5m])) / sum(rate(jimi_http_requests_total[5m])) * 100' \
  | jq '.data.result[0].value[1]'

# Via endpoint de mÃ©tricas direto
docker compose exec backend curl -s "http://prometheus:9090/api/v1/query?query=jimi_http_errors_total" | jq
```

### 4. Verificar LatÃªncia de Rede

```bash
# LatÃªncia P95 por endpoint
docker compose exec backend curl -s "http://prometheus:9090/api/v1/query" \
  --data-urlencode 'query=histogram_quantile(0.95, sum(rate(jimi_request_latency_ms_bucket[5m])) by (le, endpoint))' \
  | jq '.data.result[] | {endpoint: .metric.endpoint, p95_ms: .value[1]}'

# LatÃªncia P99
docker compose exec backend curl -s "http://prometheus:9090/api/v1/query" \
  --data-urlencode 'query=histogram_quantile(0.99, sum(rate(jimi_request_latency_ms_bucket[5m])) by (le))' \
  | jq '.data.result[0].value[1]'
```

---

## ðŸ“Š MÃ©tricas-Chave

### RequisiÃ§Ãµes por Segundo (RPS)

```promql
# RPS total
sum(rate(jimi_http_requests_total[1m]))

# RPS por endpoint
sum(rate(jimi_http_requests_total[1m])) by (endpoint)

# RPS de sucesso vs erro
sum(rate(jimi_http_requests_total{status=~"2.."}[1m]))  # Sucesso
sum(rate(jimi_http_errors_total[1m]))                    # Erro
```

### LatÃªncia (P50, P95, P99)

```promql
# P50 (mediana)
histogram_quantile(0.50, sum(rate(jimi_request_latency_ms_bucket[5m])) by (le))

# P95
histogram_quantile(0.95, sum(rate(jimi_request_latency_ms_bucket[5m])) by (le))

# P99
histogram_quantile(0.99, sum(rate(jimi_request_latency_ms_bucket[5m])) by (le))
```

### Taxa de Erro HTTP 5xx

```promql
# Porcentagem de erros
(sum(rate(jimi_http_errors_total{status=~"5.."}[5m])) / sum(rate(jimi_http_requests_total[5m]))) * 100

# Erros por endpoint
sum(rate(jimi_http_errors_total[5m])) by (endpoint, status)
```

### Tamanho da Fila de Processamento

```promql
# ConexÃµes ativas (proxy para fila)
jimi_active_connections

# Webhooks pendentes (se implementado)
# jimi_webhooks_queue_size
```

---

## ðŸ›  EstratÃ©gias de ResoluÃ§Ã£o

### 1. Aumentar Recursos (Vertical Scaling)

```yaml
# docker-compose.yml - Adicionar limites
services:
  backend:
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 256M
```

```bash
# Aplicar mudanÃ§as
docker compose up -d backend
```

### 2. Adicionar RÃ©plicas (Horizontal Scaling)

```yaml
# docker-compose.yml
services:
  backend:
    deploy:
      replicas: 3
```

```bash
# Escalar manualmente
docker compose up -d --scale backend=3
```

### 3. Implementar Circuit Breaker

**No Nginx:**
```nginx
# nginx/conf/default.conf
upstream backend {
    server backend:8000 max_fails=3 fail_timeout=30s;
    # Se houver rÃ©plicas:
    # server backend-2:8000 max_fails=3 fail_timeout=30s;
    # server backend-3:8000 max_fails=3 fail_timeout=30s;
}
```

### 4. Cache no Nginx

```nginx
# nginx/conf/default.conf
proxy_cache_path /var/cache/nginx levels=1:2 keys_zone=api_cache:10m max_size=100m inactive=60m;

location /v1/telemetry {
    # Cache para respostas idÃªnticas
    proxy_cache api_cache;
    proxy_cache_valid 200 1s;
    proxy_cache_use_stale error timeout http_500 http_502 http_503 http_504;
    
    proxy_pass http://backend;
}
```

### 5. Rate Limiting AvanÃ§ado

```nginx
# nginx/conf/nginx.conf
# Rate limit por IP
limit_req_zone $binary_remote_addr zone=per_ip:10m rate=10r/s;

# Rate limit por device_id (requer Lua ou log parsing)
# limit_req_zone $arg_device_id zone=per_device:10m rate=100r/s;

# Aplicar no location
location /v1/telemetry {
    limit_req zone=per_ip burst=20 nodelay;
    limit_req_status 429;
    # ...
}
```

---

## ðŸ’» Comandos de Troubleshooting

### Consultar Logs

```bash
# Logs do backend em tempo real
docker compose logs -f backend

# Ãšltimas 100 linhas com timestamp
docker compose logs --tail=100 -t backend

# Filtrar erros
docker compose logs backend 2>&1 | grep -i error

# Logs do Nginx
docker compose logs nginx

# Logs de todos os serviÃ§os
docker compose logs -f
```

### Queries Prometheus

```bash
# Verificar targets ativos
docker compose exec backend curl -s "http://prometheus:9090/api/v1/targets" | jq '.data.activeTargets[] | {job: .labels.job, health: .health}'

# Listar todas as mÃ©tricas disponÃ­veis
docker compose exec backend curl -s "http://prometheus:9090/api/v1/label/__name__/values" | jq '.data[]' | grep jimi

# Query de range (Ãºltimas 5 minutos)
docker compose exec backend curl -s "http://prometheus:9090/api/v1/query_range" \
  --data-urlencode 'query=rate(jimi_webhooks_received_total[1m])' \
  --data-urlencode 'start='$(date -d '5 minutes ago' +%s) \
  --data-urlencode 'end='$(date +%s) \
  --data-urlencode 'step=15s' | jq
```

### Queries Loki

```bash
# Logs de erro do backend
docker compose exec backend curl -s "http://loki:3100/loki/api/v1/query_range" \
  --data-urlencode 'query={job="backend"} |= "error"' \
  --data-urlencode 'limit=50' | jq '.data.result[].values[]'

# Logs por device_id especÃ­fico
docker compose exec backend curl -s "http://loki:3100/loki/api/v1/query_range" \
  --data-urlencode 'query={job="backend"} |= "JIMI-001"' \
  --data-urlencode 'limit=20' | jq

# Contar erros por minuto
docker compose exec backend curl -s "http://loki:3100/loki/api/v1/query" \
  --data-urlencode 'query=count_over_time({job="backend"} |= "error" [1m])' | jq
```

### Analisar Healthchecks

```bash
# Status detalhado de um container
docker inspect jimi-backend --format='{{json .State.Health}}' | jq

# Verificar health de todos os containers
for c in $(docker compose ps -q); do
  name=$(docker inspect $c --format='{{.Name}}')
  health=$(docker inspect $c --format='{{.State.Health.Status}}' 2>/dev/null || echo "no healthcheck")
  echo "$name: $health"
done
```

---

## ðŸ“– Runbooks

### Runbook 1: Backend NÃ£o Responde

```bash
#!/bin/bash
# runbook-backend-down.sh

echo "ðŸ” DiagnÃ³stico: Backend Down"

# 1. Verificar status
echo "1. Status do container:"
docker compose ps backend

# 2. Verificar logs recentes
echo -e "\n2. Ãšltimos logs:"
docker compose logs --tail=50 backend

# 3. Verificar recursos
echo -e "\n3. Uso de recursos:"
docker stats --no-stream jimi-backend

# 4. Tentar restart
echo -e "\n4. Tentando restart..."
docker compose restart backend

# 5. Verificar novamente
sleep 10
echo -e "\n5. Status apÃ³s restart:"
docker compose ps backend

# 6. Health check
echo -e "\n6. Health check:"
docker compose exec backend curl -s http://localhost:8000/health || echo "FALHA"
```

### Runbook 2: Alta LatÃªncia

```bash
#!/bin/bash
# runbook-high-latency.sh

echo "ðŸ” DiagnÃ³stico: Alta LatÃªncia"

# 1. Verificar P95 atual
echo "1. LatÃªncia P95:"
docker compose exec backend curl -s "http://prometheus:9090/api/v1/query" \
  --data-urlencode 'query=histogram_quantile(0.95, sum(rate(jimi_request_latency_ms_bucket[5m])) by (le))' \
  | jq '.data.result[0].value[1]'

# 2. Verificar recursos
echo -e "\n2. Uso de recursos:"
docker stats --no-stream --format "table {{.Name}}\t{{.CPUPerc}}\t{{.MemPerc}}"

# 3. Verificar conexÃµes ativas
echo -e "\n3. ConexÃµes ativas:"
docker compose exec backend curl -s "http://prometheus:9090/api/v1/query?query=jimi_active_connections" \
  | jq '.data.result[0].value[1]'

# 4. RPS atual
echo -e "\n4. RPS atual:"
docker compose exec backend curl -s "http://prometheus:9090/api/v1/query" \
  --data-urlencode 'query=sum(rate(jimi_http_requests_total[1m]))' \
  | jq '.data.result[0].value[1]'

# 5. RecomendaÃ§Ãµes
echo -e "\nðŸ“‹ RecomendaÃ§Ãµes:"
echo "   - Se CPU > 70%: Considerar escalar horizontalmente"
echo "   - Se MemÃ³ria > 80%: Aumentar limites ou verificar memory leaks"
echo "   - Se RPS muito alto: Verificar rate limiting"
```

### Runbook 3: Disco Cheio (Logs/MÃ©tricas)

```bash
#!/bin/bash
# runbook-disk-full.sh

echo "ðŸ” DiagnÃ³stico: EspaÃ§o em Disco"

# 1. Verificar uso de disco
echo "1. Uso de disco:"
df -h /var/lib/docker

# 2. Volumes Docker
echo -e "\n2. Volumes Docker:"
docker system df -v

# 3. Logs grandes
echo -e "\n3. Tamanho dos logs:"
du -sh /var/lib/docker/containers/*/

# 4. Limpar logs (cuidado!)
echo -e "\n4. Para limpar logs antigos:"
echo "   docker compose logs --no-log-prefix backend | tail -n 10000 > /tmp/backend-recent.log"
echo "   truncate -s 0 /var/lib/docker/containers/<container-id>/*-json.log"

# 5. Prune de recursos nÃ£o utilizados
echo -e "\n5. Para limpar recursos nÃ£o utilizados:"
echo "   docker system prune -a --volumes"
```

---

## ðŸ“ˆ Dashboards de ReferÃªncia

### Grafana - Queries Ãšteis

**Webhook Success Rate:**
```promql
(sum(rate(jimi_webhooks_received_total{status="success"}[5m])) / sum(rate(jimi_webhooks_received_total[5m]))) * 100
```

**Throughput por Endpoint:**
```promql
sum(rate(jimi_webhooks_received_total[1m])) by (endpoint)
```

**Error Budget Consumption:**
```promql
1 - (sum(rate(jimi_http_requests_total{status=~"2.."}[24h])) / sum(rate(jimi_http_requests_total[24h])))
```

---

## ðŸ“ž EscalaÃ§Ã£o

| NÃ­vel | CritÃ©rio | AÃ§Ã£o |
|-------|----------|------|
| **L1** | Taxa de erro < 5%, LatÃªncia P95 < 500ms | Monitorar |
| **L2** | Taxa de erro 5-10%, LatÃªncia P95 500ms-1s | Investigar + Notificar |
| **L3** | Taxa de erro > 10%, LatÃªncia P95 > 1s | Incidente + Escalar |
| **Critical** | Backend down, Perda de dados | War room imediato |

---

**Ãšltima atualizaÃ§Ã£o:** Fevereiro 2025
